import * as readline from 'readline';
import { exec } from 'child_process';
import { promisify } from 'util';
import * as fs from 'fs';
import * as path from 'path';
import * as dotenv from 'dotenv';
import { GoogleGenerativeAI } from '@google/generative-ai';
import OpenAI from 'openai';
import TOML from '@iarna/toml';

dotenv.config();
const execAsync = promisify(exec);

const SYSTEM_INSTRUCTION = `
ã‚ãªãŸã¯ STELLA (Stateful Turn-based Execution & LLM Loop Architecture) ã®ä¸­æ ¸ã¨ãªã‚‹é ­è„³ï¼ˆCPUï¼‰ã§ã™ã€‚
ã‚ãªãŸã¯ã€Œæ°¸ç¶šçš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ã§ã‚ã‚Šã€ä¸€ã¤ã®ã‚¿ã‚¹ã‚¯ãŒçµ‚ã‚ã£ã¦ã‚‚å‹æ‰‹ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ï¼ˆfinishï¼‰ã›ãšã€å ±å‘Šã®å¾Œã«æ¬¡ã®æŒ‡ç¤ºã‚’å¾…ã¤ã®ãŒåŸºæœ¬ã‚¹ã‚¿ã‚¤ãƒ«ã§ã™ã€‚

ã€å‡ºåŠ›ãƒ«ãƒ¼ãƒ«ã€‘
ã‚ãªãŸã¯ã€Œæ€è€ƒå†…å®¹ã€ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ã€Œãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã€ã‚’ä»¥ä¸‹ã®JSONå½¢å¼ã§ã®ã¿å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
æ±ºã—ã¦JSONä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚

{
  "thought": "ç¾åœ¨ã®çŠ¶æ³åˆ†æã¨æ¬¡ã«è¡Œã†ã¹ãã“ã¨ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹",
  "message": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ã€è¦ç´„çµæœã€å ±å‘Šäº‹é …",
  "tool": "å®Ÿè¡Œã™ã‚‹ãƒ„ãƒ¼ãƒ«åï¼ˆä¸è¦ãªå ´åˆã¯ç©ºæ–‡å­— \"\"ï¼‰",
  "parameters": { "å¼•æ•°å": "å€¤" }
}

ã€åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã€‘
1. run_shell: ã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
2. ask_user: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è³ªå•ã—ã¾ã™ã€‚
3. list_models: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚
4. switch_model: { "model_name": "...", "provider": "google|github|openai" } ã§è¨­å®šã‚’å¤‰æ›´ã—ã¾ã™ã€‚
5. finish: ã‚¢ãƒ—ãƒªã‚’çµ‚äº†ã—ã¾ã™ã€‚
`;

interface ChatSession {
  sendMessage(input: string): Promise<string>;
}

class GeminiChatSession implements ChatSession {
  private chat: any;
  constructor(apiKey: string, modelName: string) {
    const genAI = new GoogleGenerativeAI(apiKey);
    const model = genAI.getGenerativeModel({ model: modelName, systemInstruction: SYSTEM_INSTRUCTION });
    this.chat = model.startChat({ history: [] });
  }
  async sendMessage(input: string): Promise<string> {
    const res = await this.chat.sendMessage(input);
    return res.response.text();
  }
}

class OpenAIChatSession implements ChatSession {
  private client: OpenAI; private modelName: string; private history: any[] = [];
  constructor(apiKey: string, modelName: string, baseURL?: string) {
    this.client = new OpenAI({ apiKey, baseURL });
    this.modelName = modelName;
    this.history.push({ role: 'system', content: SYSTEM_INSTRUCTION });
  }
  async sendMessage(input: string): Promise<string> {
    this.history.push({ role: 'user', content: input });
    const res = await this.client.chat.completions.create({
      model: this.modelName, messages: this.history, response_format: { type: "json_object" }
    });
    const text = res.choices[0].message.content || "{}";
    this.history.push({ role: 'assistant', content: text });
    return text;
  }
}

const CONFIG_PATH = path.join(process.cwd(), 'config.toml');
let config: any = { model_name: "gemini-2.0-flash-lite-preview-02-05", provider: "google" };
if (fs.existsSync(CONFIG_PATH)) config = { ...config, ...TOML.parse(fs.readFileSync(CONFIG_PATH, 'utf-8')) };

const DEBUG = process.env.STELLA_DEBUG === 'true';

async function main() {
  const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
  const q = (msg: string): Promise<string> => new Promise(r => rl.question(msg, r));

  let session: ChatSession;
  try {
    if (config.provider === "google") {
      session = new GeminiChatSession(process.env.GEMINI_API_KEY || "", config.model_name);
    } else if (config.provider === "github") {
      session = new OpenAIChatSession(process.env.GITHUB_TOKEN || "", config.model_name, "https://models.inference.ai.azure.com");
    } else if (config.provider === "openai") {
      session = new OpenAIChatSession(process.env.OPENAI_API_KEY || "", config.model_name); // Default is OpenAI API
    } else {
      throw new Error(`æœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ${config.provider}`);
    }
  } catch (e: any) {
    console.error("èµ·å‹•ã‚¨ãƒ©ãƒ¼:", e.message); process.exit(1);
  }

  console.log(`ğŸŒŸ STELLA (${config.model_name} / ${config.provider}) èµ·å‹•`);
  let nextInput: string = await q("æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ");

  while (true) {
    try {
      if (DEBUG) {
        console.log("\n--- [DEBUG] API Request ---");
        console.log(nextInput);
        console.log("---------------------------\n");
      }

      const respText = await session.sendMessage(nextInput);

      if (DEBUG) {
        console.log("\n--- [DEBUG] API Response ---");
        console.log(respText);
        console.log("----------------------------\n");
      }

      const call = JSON.parse(respText.substring(respText.indexOf('{'), respText.lastIndexOf('}') + 1));

      if (call.thought) console.log(`ğŸ§  [æ€è€ƒ]: ${call.thought}`);
      if (call.message) console.log(`------------------\nğŸ’¬ [STELLA]: ${call.message}\n------------------`);
      if (!call.tool) { nextInput = await q("æ¬¡ã¯ä½•ã‚’ã—ã¾ã™ã‹ï¼Ÿ: "); continue; }

      console.log(`ğŸ› ï¸ [å®Ÿè¡Œ]: ${call.tool}`);
      let result = "";
      switch (call.tool) {
        case 'run_shell':
          const { stdout, stderr } = await execAsync(call.parameters.command);
          const out = stdout.trim() || stderr.trim();
          if (out) console.log(`   > å®Ÿè¡Œçµæœ(ä¸€éƒ¨):\n${out.split('\n').slice(0, 5).join('\n')}${out.split('\n').length > 5 ? '\n...' : ''}`);
          result = `[stdout]\n${stdout}\n[stderr]\n${stderr}`;
          break;
        case 'ask_user':
          result = `[User Answer]: ${await q(`â“ ${call.parameters.query}\nå›ç­”: `)}`;
          break;
        case 'switch_model':
          config = { ...config, ...call.parameters };
          fs.writeFileSync(CONFIG_PATH, TOML.stringify(config));
          console.log("âœ… è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚å†èµ·å‹•ãŒå¿…è¦ã§ã™ã€‚");
          process.exit(0);
        case 'list_models':
          if (config.provider === "google") {
            const resp = await fetch(`https://generativelanguage.googleapis.com/v1beta/models?key=${process.env.GEMINI_API_KEY}`);
            const data = await resp.json() as any;
            result = `Gemini models: ${data.models.map((m: any) => m.name.replace("models/", "")).join(", ")}`;
          } else {
            try {
              const res = await fetch("https://models.inference.ai.azure.com/models", {
                headers: { "Authorization": `Bearer ${process.env.GITHUB_TOKEN}` }
              });
              const data = await res.json() as any[];
              const modelIds = data.map(m => m.name);
              result = `GitHub Models: ${modelIds.join(", ")}`;
            } catch (e: any) {
              result = `GitHub Models ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: ${e.message}`;
            }
          }
          break;
        case 'finish': process.exit(0);
        default: result = "æœªçŸ¥ã®ãƒ„ãƒ¼ãƒ«";
      }
      nextInput = `ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ:\n${result}`;
    } catch (e: any) {
      console.error("âŒ Error:", e.message);
      nextInput = `ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${e.message}. å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚`;
    }
  }
}
main().catch(console.error);
